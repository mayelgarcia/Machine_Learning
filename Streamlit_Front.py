import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from wordcloud import WordCloud
from PIL import Image

from stop_words import get_stop_words

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB

import warnings
warnings.filterwarnings("ignore")

import tqdm
import time

from sklearn.pipeline import make_pipeline

# Used to standardize the predictors
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestClassifier


# Method to apply logistic regression
from sklearn.linear_model import LogisticRegression


# Importing the method needed to apply KNN classification

from sklearn.neighbors import KNeighborsClassifier

import streamlit as st

st.title("Fake-News Classification Using Text-Data")

st.header("First 10 columns of DataFrame")
my_df = pd.read_csv('Dataset_noText.csv')

st.header("First 10 columns of DataFrame")
st.table(my_df.iloc[0:10])

st.markdown('The first interesting thing is similar the most frequently used words are for both Fake and Real news. This is affected mainly by trending topics.')


images = ['images/fake_wc.png', 'images/real_wc.png']
st.image(images, width=340, caption=["WordCloud generated by querying the most frequent works using in Fake news and articles ", "WordCloud generated by querying the most frequent works using in Real news and articles"] )

st.header("Emotions displayed in texts")

st.markdown('The "emotion" rate is a value between 0 and 1 which represents the presence of a determined emotion in a text. "Main emotion" shows the higher emotion value.')
st.image("images/emotions.png")

st.markdown('They are quite similar. However, it is clear there is a greater proportion for anger as the main emotion of Fake News when compared to real news. This values were obtained training a Neural Network.')



st.header("Distribution of monosyllabic and polysyllabic words")

st.image("images/monopoly.png")

st.markdown("This bar chart represents the distribution of the ammount of monosyllabic and polysyllabic words present on Real anf Fake news articles and headers; it's curious how, generally speaking, there is a higher ammount of Real news on the lower ammount of monosyllabic words when compared to Fake News. ")

st.header("Modeling")

st.markdown("The following is a list of the  binary classification models evaluated, along with their respective classification report; they are sorted by the model's accuracy:")

st.header("**1. Decision tree**")

st.image("images/classtree.png")

st.markdown("A classification tree is a structural mapping of binary decisions that lead to a decision about the class of an object. Although sometimes referred to as a decision tree, it is more properly a type of decision tree that leads to categorical decisions. After pre-processing the categorical variables and splitting the training data and test data in a proportion of 80/20, the results of the classification are obtained for a tree before “prunning”.")

st.markdown("""
|  	| precision 	| recall 	| f1-score 	| support 	|
|---	|---	|---	|---	|---	|
| 0 	| 0.53 	| 0.65 	| 0.59 	| 19884 	|
| 1 	| 0.57 	| 0.44 	| 0.50 	| 20513 	|
|  	|  	|  	|  	|  	|
| accuracy 	|  	|  	| 0.55 	| 40397 	|
| macro avg 	| 0.55 	| 0.55 	| 0.54 	| 40397 	|
| weighted avg 	| 0.55 	| 0.55 	| 0.54 	| 40397 	|
""")

st.header("**2. Logistic regression**")

st.markdown("The mathematical concept of logistic regression is to express the relationship between outcome variable and predictor variables (independent variables) in terms of logit: The natural logarithm of odds.")

st.markdown("""
|  	| precision 	| recall 	| f1-score 	| support 	|
|---	|---	|---	|---	|---	|
| 0 	| 0.53 	| 0.49 	| 0.51 	| 19884 	|
| 1 	| 0.54 	| 0.58 	| 0.56 	| 20513 	|
|  	|  	|  	|  	|  	|
| accuracy 	|  	|  	| 0.54 	| 40397 	|
| macro avg 	| 0.54 	| 0.54 	| 0.53 	| 40397 	|
| weighted avg 	| 0.54 	| 0.54 	| 0.54 	| 40397 	|
""")

st.markdown("After tuning parameters using RandomizedSearchCV, the resulting model was the exact same in terms of the classification report.")







st.header("**3. Naive Bayes**")

st.markdown("Naive Bayes is a technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. ")

st.markdown("""
|  	| precision 	| recall 	| f1-score 	| support 	|
|---	|---	|---	|---	|---	|
| 0 	| 0.53 	| 0.23 	| 0.32 	| 19884 	|
| 1 	| 0.52 	| 0.80 	| 0.63 	| 20513 	|
|  	|  	|  	|  	|  	|
| accuracy 	|  	|  	| 0.52 	| 40397 	|
| macro avg 	| 0.52 	| 0.52 	| 0.47 	| 40397 	|
| weighted avg 	| 0.52 	| 0.52 	| 0.48 	| 40397 	|
""")

st.markdown("It is noteworthy the higher recall when detecting Fake News, but how low it is when classifying Real News")


st.header("**4. K-Nearest Neighbors**")

st.markdown("A type of classification where the function is only approximated locally and all computation is deferred until function evaluation.")

st.markdown("""
|  	| precision 	| recall 	| f1-score 	| support 	|
|---	|---	|---	|---	|---	|
| 0 	| 0.46 	| 0.45 	| 0.46 	| 19884 	|
| 1 	| 0.48 	| 0.48 	| 0.48 	| 20513 	|
|  	|  	|  	|  	|  	|
| accuracy 	|  	|  	| 0.47 	| 40397 	|
| macro avg 	| 0.47 	| 0.47 	| 0.47 	| 40397 	|
| weighted avg 	| 0.47 	| 0.47 	| 0.47 	| 40397 	|
""")

st.markdown("An important thing to consider is the high dimensionality of this data; both the high ammount of predictors and the high ammount of obseravtions hinder the K-Nearest Neighbors algorithm.")




st.header("**5. Bagging**")

st.markdown("Bagging, also known as bootstrap aggregation, is a learning method that is commonly used to reduce variance within a noisy dataset.")

st.markdown("""
|  	| precision 	| recall 	| f1-score 	| support 	|
|---	|---	|---	|---	|---	|
| 0 	| 0.36 	| 0.35	| 0.36 	| 19884 	|
| 1 	| 0.38 	| 0.38 	| 0.38 	| 20513 	|
|  	|  	|  	|  	|  	|
| accuracy 	|  	|  	| 0.37	| 40397 	|
| macro avg 	| 0.37 	| 0.37 	| 0.37 	| 40397 	|
| weighted avg 	| 0.37 	| 0.37 	| 0.37 	| 40397 	|
""")

st.markdown("Surprisingly, this model took the longest time to fit.")

st.header("**6. Random Forest**")

st.markdown("Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned.")

st.markdown("""
|  	| precision 	| recall 	| f1-score 	| support 	|
|---	|---	|---	|---	|---	|
| 0 	| 0.36 	| 0.35	| 0.36 	| 19884 	|
| 1 	| 0.38 	| 0.39 	| 0.38 	| 20513 	|
|  	|  	|  	|  	|  	|
| accuracy 	|  	|  	| 0.37	| 40397 	|
| macro avg 	| 0.37 	| 0.37 	| 0.37 	| 40397 	|
| weighted avg 	| 0.37 	| 0.37 	| 0.37 	| 40397 	|
""")

st.header("Conclusion")

st.markdown("Six classification methods were used to perform a classificatory analysis of fake news and made predictions. We present our work and demonstrate the advantages of the data mining techniques including logistic regression and decision tree to fake news detection and classification. Taking the overall model accuracy as a criterion, the models with a better performance were the classification tree and the logistic regression models, both with an accuracy of 0.55 and 0.54 respectively. ")